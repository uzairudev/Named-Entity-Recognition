{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4je-ekC1l6iU"
   },
   "source": [
    "# Hidden Markov Model project\n",
    "## Machine Learning Fundamentals\n",
    "\n",
    "|First name|Last name|Master program|Contribution|\n",
    "|----------|---------|--------------|-------------|\n",
    "|Hoang Nam|Nguyen|IMLEX|33.3%|\n",
    "|Chihiro|Tone|IMLEX|33.3%|\n",
    "|Uzairu|Abubakar|IMLEX|33.3%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# WARNING : you need version 0.2.6 of hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9554,
     "status": "ok",
     "timestamp": 1677665807645,
     "user": {
      "displayName": "Richard Ser",
      "userId": "01935729326923979164"
     },
     "user_tz": -60
    },
    "id": "-cnVa0zdl-0T",
    "outputId": "3830e7b9-1012-4db9-916e-0428cc77dc42",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hmmlearn==0.2.6\n",
      "  Using cached hmmlearn-0.2.6-cp310-cp310-linux_x86_64.whl\n",
      "Collecting scikit-learn>=0.16\n",
      "  Using cached scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "Collecting numpy>=1.10\n",
      "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting scipy>=0.19\n",
      "  Using cached scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn, hmmlearn\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.1.0\n",
      "    Uninstalling threadpoolctl-3.1.0:\n",
      "      Successfully uninstalled threadpoolctl-3.1.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.2.0\n",
      "    Uninstalling joblib-1.2.0:\n",
      "      Successfully uninstalled joblib-1.2.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: hmmlearn\n",
      "    Found existing installation: hmmlearn 0.2.6\n",
      "    Uninstalling hmmlearn-0.2.6:\n",
      "      Successfully uninstalled hmmlearn-0.2.6\n",
      "Successfully installed hmmlearn-0.2.6 joblib-1.2.0 numpy-1.24.3 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall hmmlearn==0.2.6\n",
    "import hmmlearn\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/tc11802x/.local/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /home/tc11802x/.local/lib/python3.10/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/tc11802x/.local/lib/python3.10/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/lib/python3/dist-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=0.25->seaborn) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/python3/dist-packages (from pandas>=0.25->seaborn) (2.8.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/tc11802x/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/tc11802x/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/tc11802x/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/tc11802x/.local/lib/python3.10/site-packages (from nltk) (2023.3.23)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Name: hmmlearn\n",
      "Version: 0.2.6\n",
      "Summary: Hidden Markov Models in Python with scikit-learn like API\n",
      "Home-page: https://github.com/hmmlearn/hmmlearn\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: /home/tc11802x/.local/lib/python3.10/site-packages\n",
      "Requires: numpy, scikit-learn, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install nltk\n",
    "!pip show hmmlearn  # check that the version installed is 0.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING : check that the hmmlearn installed is version 0.2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tyj9Ac1Wlz1m"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt  # show graph\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEiMw2lwmhy1"
   },
   "source": [
    "## In this notebook we will look at the NER dataset and use it to understand HMM and also construct a POS tagger at the same time.\n",
    "\n",
    "### Data Description:\n",
    "#### sentence: this column donates to which sentence the word belongs\n",
    "#### Word: the word in the sentence\n",
    "#### POS: Associated POS tag for the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GARTR2z4scYU"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_wSo_y2sv6f"
   },
   "source": [
    "Otherwise, if you are working locally or if you just uploaded the dataset for this session on the drive, you can use the following to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vK3Tsr14s8Oi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1          NaN             of   IN      O\n",
       "2          NaN  demonstrators  NNS      O\n",
       "3          NaN           have  VBP      O\n",
       "4          NaN        marched  VBN      O\n",
       "5          NaN        through   IN      O\n",
       "6          NaN         London  NNP  B-geo\n",
       "7          NaN             to   TO      O\n",
       "8          NaN        protest   VB      O\n",
       "9          NaN            the   DT      O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data = pd.read_csv(\"./data/NER dataset.csv\", encoding='latin1')\n",
    "_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YV2vxuIdmGW0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1  Sentence: 1             of   IN      O\n",
       "2  Sentence: 1  demonstrators  NNS      O\n",
       "3  Sentence: 1           have  VBP      O\n",
       "4  Sentence: 1        marched  VBN      O\n",
       "5  Sentence: 1        through   IN      O\n",
       "6  Sentence: 1         London  NNP  B-geo\n",
       "7  Sentence: 1             to   TO      O\n",
       "8  Sentence: 1        protest   VB      O\n",
       "9  Sentence: 1            the   DT      O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data = _data.fillna(method=\"ffill\")\n",
    "_data = _data.rename(columns={'Sentence #': 'sentence'})\n",
    "_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bieothyVtLxy"
   },
   "source": [
    "# Data pre-processing\n",
    "If you want to do some pre-processing (lowercase any words, remove stop words, replace numbers/names by a unique NUM/NAME token, etc.) you can do it here in the pipeline.\n",
    "\n",
    "Note : you could create a new dataset `data_pre_precessed = pre_process(data)` to keep both version and compare the effect of you pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "txb_5PPstsKc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tc11802x/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def pre_processing(src):\n",
    "    processed_data = src.copy(deep=True)\n",
    "    processed_data.Word = processed_data.Word.str.lower()\n",
    "    processed_data.Word = processed_data.Word.apply(lambda x: re.sub(r'\\d+(\\.*,*/*-*\\d+)?', 'NUM', x))\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # processed_data['Word'] = processed_data['Word'].apply(lambda x: x if x not in stop_words else '')\n",
    "    processed_data = processed_data.reset_index(drop=True)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS    Tag\n",
       "0  Sentence: 1      thousands  NNS      O\n",
       "1  Sentence: 1             of   IN      O\n",
       "2  Sentence: 1  demonstrators  NNS      O\n",
       "3  Sentence: 1           have  VBP      O\n",
       "4  Sentence: 1        marched  VBN      O\n",
       "5  Sentence: 1        through   IN      O\n",
       "6  Sentence: 1         london  NNP  B-geo\n",
       "7  Sentence: 1             to   TO      O\n",
       "8  Sentence: 1        protest   VB      O\n",
       "9  Sentence: 1            the   DT      O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pre_processing(_data)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIPM3ZblnIfB"
   },
   "source": [
    "First let's collect the unique words and the unique POS tags in the dataset, we will use this to construct the HMM later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1677617727116,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "z8ztujKumbQb",
    "outputId": "a49eec7c-0a8d-46f4-8299-a183f9d5c447"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 29876)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(data.POS.values))  # Unique POS tags in the dataset\n",
    "words = list(set(data.Word.values))  # Unique words in the dataset\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXihLYTNnV84"
   },
   "source": [
    "### We have 42 different tags and 35,178 different words, so the HMM that we construct will have the following properties\n",
    "- The hidden states of the this HMM will correspond to the POS tags, so we will have 42 hidden states.\n",
    "- The Observations for this HMM will correspond to the sentences and their words.\n",
    "\n",
    "#### Before constructing the HMM, we will split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3Xb_rvZ6nQNH"
   },
   "outputs": [],
   "source": [
    "y = data.POS\n",
    "X = data.drop('POS', axis=1)\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix, test_ix = next(gs.split(X, y, groups=data['sentence']))\n",
    "\n",
    "data_train = data.loc[train_ix]\n",
    "data_test = data.loc[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FhZFM-48t3OI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence      Word  POS Tag\n",
       "24  Sentence: 2  families  NNS   O\n",
       "25  Sentence: 2        of   IN   O\n",
       "26  Sentence: 2  soldiers  NNS   O\n",
       "27  Sentence: 2    killed  VBN   O\n",
       "28  Sentence: 2        in   IN   O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FMrZCuzut6ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>british</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Sentence: 7</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Sentence: 7</td>\n",
       "      <td>london</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Sentence: 7</td>\n",
       "      <td>march</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Sentence: 7</td>\n",
       "      <td>came</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Sentence: 7</td>\n",
       "      <td>ahead</td>\n",
       "      <td>RB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Sentence: 7</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence           Word  POS    Tag\n",
       "0    Sentence: 1      thousands  NNS      O\n",
       "1    Sentence: 1             of   IN      O\n",
       "2    Sentence: 1  demonstrators  NNS      O\n",
       "3    Sentence: 1           have  VBP      O\n",
       "4    Sentence: 1        marched  VBN      O\n",
       "5    Sentence: 1        through   IN      O\n",
       "6    Sentence: 1         london  NNP  B-geo\n",
       "7    Sentence: 1             to   TO      O\n",
       "8    Sentence: 1        protest   VB      O\n",
       "9    Sentence: 1            the   DT      O\n",
       "10   Sentence: 1            war   NN      O\n",
       "11   Sentence: 1             in   IN      O\n",
       "12   Sentence: 1           iraq  NNP  B-geo\n",
       "13   Sentence: 1            and   CC      O\n",
       "14   Sentence: 1         demand   VB      O\n",
       "15   Sentence: 1            the   DT      O\n",
       "16   Sentence: 1     withdrawal   NN      O\n",
       "17   Sentence: 1             of   IN      O\n",
       "18   Sentence: 1        british   JJ  B-gpe\n",
       "19   Sentence: 1         troops  NNS      O\n",
       "20   Sentence: 1           from   IN      O\n",
       "21   Sentence: 1           that   DT      O\n",
       "22   Sentence: 1        country   NN      O\n",
       "23   Sentence: 1              .    .      O\n",
       "132  Sentence: 7            the   DT      O\n",
       "133  Sentence: 7         london  NNP  B-geo\n",
       "134  Sentence: 7          march   NN      O\n",
       "135  Sentence: 7           came  VBD      O\n",
       "136  Sentence: 7          ahead   RB      O\n",
       "137  Sentence: 7             of   IN      O"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu1fwHIzo0KU"
   },
   "source": [
    "Now lets encode the POS and Words to be used to generate the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 570,
     "status": "ok",
     "timestamp": 1677617748855,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "yj8cnwcOoznK",
    "outputId": "5d4c0212-d217-4b6f-df13-ae6267a46b16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 23720)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\n",
    "dfupdate.Word = 'UNKNOWN' # hide 15% of words\n",
    "data_train.update(dfupdate)\n",
    "words = list(set(data_train.Word.values))\n",
    "# Convert words and tags into numbers\n",
    "word2id = {w: i for i, w in enumerate(words)}\n",
    "tag2id = {t: i for i, t in enumerate(tags)}\n",
    "id2tag = {i: t for i, t in enumerate(tags)}\n",
    "len(tags), len(words) # variation of the words should be decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koiN38BSpNZb"
   },
   "source": [
    "In your theory classes you might have seen that the Hidden Markov Models can be learned by using the Baum-Welch algorithm by just using the observations.\n",
    "Although we can learn the Hidden States (POS tags) using Baum-Welch algorithm,We cannot map them back the states (words) to the POS tag. So for this exercise we will skip using the BW algorithm and directly create the HMM.\n",
    "\n",
    "For creating the HMM we should build the following three parameters. \n",
    "- `startprob_`\n",
    "- `transmat_`\n",
    "- `emissionprob_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RXWyEXlqD0B"
   },
   "source": [
    "To construct the above mentioned paramters let's first create some useful matrices that will assist us in creating the above three parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uiXtl641o76N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702936/702936 [00:00<00:00, 918873.04it/s]\n"
     ]
    }
   ],
   "source": [
    "count_tags = dict(data_train.POS.value_counts())  # Total number of POS tags in the dataset\n",
    "# Now let's create the tags to words count\n",
    "count_tags_to_words = data_train.groupby(['POS']).apply(\n",
    "    lambda grp: grp.groupby('Word')['POS'].count().to_dict()).to_dict()\n",
    "# We shall also collect the counts for the first tags in the sentence\n",
    "count_init_tags = dict(data_train.groupby('sentence').first().POS.value_counts())\n",
    "\n",
    "# Create a mapping that stores the frequency of transitions in tags to it's next tags\n",
    "count_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\n",
    "sentences = list(data_train.sentence)\n",
    "pos = list(data_train.POS)\n",
    "\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        prevtagid = tag2id[pos[i - 1]]\n",
    "        nexttagid = tag2id[pos[i]]\n",
    "        count_tags_to_next_tags[prevtagid][nexttagid] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S67MQmn5sCWJ"
   },
   "source": [
    "Now Let's build the parameter matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LPV6pioBqaey"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:01<00:00, 40.76it/s]\n"
     ]
    }
   ],
   "source": [
    "startprob = np.zeros((len(tags),))\n",
    "transmat = np.zeros((len(tags), len(tags)))\n",
    "emissionprob = np.zeros((len(tags), len(words)))\n",
    "num_sentences = sum(count_init_tags.values())\n",
    "sum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\n",
    "for tag, tagid in tqdm(tag2id.items(), position=0, leave=True):\n",
    "    floatCountTag = float(count_tags.get(tag, 0))\n",
    "    startprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n",
    "    for word, wordid in word2id.items():\n",
    "        emissionprob[tagid][wordid] = count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n",
    "    for tag2, tagid2 in tag2id.items():\n",
    "        transmat[tagid][tagid2] = count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sbca__ogusIu"
   },
   "source": [
    "## Task 1: Similar to how we built the hidden state transition probability matrix as shown above, you will built the transition probability between the words. With this matrix write a function that can calculate the log likelihood given a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 702936/702936 [00:01<00:00, 433728.52it/s]\n",
      "100%|██████████| 23720/23720 [06:39<00:00, 59.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the word transition matrix, this matrix will be of shape words x words\n",
    "count_words_to_next_words = np.zeros((len(words), len(words)), dtype=int)\n",
    "words_ = list(data_train.Word)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        prevwordid = word2id[words_[i - 1]]\n",
    "        nextwordid = word2id[words_[i]]\n",
    "        count_words_to_next_words[prevwordid][nextwordid] += 1\n",
    "\n",
    "word_transition_matrix = np.zeros((len(words), len(words)))\n",
    "sum_words_to_next_words = np.sum(count_words_to_next_words, axis=1)\n",
    "for word, wordid in tqdm(word2id.items(), position=0, leave=True):\n",
    "    if sum_words_to_next_words[wordid] == 0:\n",
    "        continue\n",
    "    for word2, wordid2 in word2id.items():\n",
    "        word_transition_matrix[wordid][wordid2] = count_words_to_next_words[wordid][wordid2] / sum_words_to_next_words[wordid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2idlist(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'\\d+(\\.*,*/*-*\\d+)?', 'NUM', sentence)\n",
    "    return [word2id[w] for w in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_likelihood(sentence, word_transition_matrix) -> float:\n",
    "    \"\"\"\n",
    "    Write a function given a sentence and transition_matrix will return the loglikehood of the sentence\n",
    "    \"\"\"\n",
    "    l = 0\n",
    "    for i, wordid in enumerate(sentence):\n",
    "        if i == 0:\n",
    "            prevwordid = wordid\n",
    "            continue\n",
    "        l += np.log(word_transition_matrix[prevwordid][wordid])\n",
    "        prevwordid = wordid\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-35.11892547319604"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate_log_likelihood(sentence2idlist(\"This is a test sentence\"), word_transition_matrix)\n",
    "# calculate_log_likelihood(sentence2idlist(\"Who are you ?\"), word_transition_matrix)\n",
    "# calculate_log_likelihood(sentence2idlist(\"This is a protest about how the new law is not in the interest of the people\"), word_transition_matrix)\n",
    "# calculate_log_likelihood(sentence2idlist(\"You are not me\"), word_transition_matrix)\n",
    "calculate_log_likelihood(sentence2idlist(\"do you expect to be happy to work late\"), word_transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edf1YAFvwgXV"
   },
   "source": [
    "#### Now we will continue to constructing the HMM.\n",
    "\n",
    "We will use the hmmlearn implementation to initialize the HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IjanyLMiwzPa"
   },
   "outputs": [],
   "source": [
    "model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi', random_state=42)\n",
    "model.startprob_ = startprob # pi\n",
    "model.transmat_ = transmat # A\n",
    "model.emissionprob_ = emissionprob # B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYDvk4HEw_SO"
   },
   "source": [
    "#### Before using the HMM to predict the POS tags, we have to fix the training set as some of the words and tags in the test data might not appear in the training data so we collect this data to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "v_GGsS_-w-fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 345639/345639 [00:00<00:00, 1139412.11it/s]\n"
     ]
    }
   ],
   "source": [
    "data_test.loc[~data_test['Word'].isin(words), 'Word'] = 'UNKNOWN'\n",
    "word_test = list(data_test.Word)\n",
    "samples = []\n",
    "for i, val in enumerate(word_test):\n",
    "    samples.append([word2id[val]])\n",
    "\n",
    "# TODO use pandas solution\n",
    "lengths = []\n",
    "count = 0\n",
    "sentences = list(data_test.sentence)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        count += 1\n",
    "    elif i > 0:\n",
    "        lengths.append(count)\n",
    "        count = 1\n",
    "    else:\n",
    "        count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3UGvRN9x2fn"
   },
   "source": [
    "Now that we have the HMM ready lets predict the best path from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181278,
     "status": "ok",
     "timestamp": 1677617955431,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "_5PGjZaXx6xS",
    "outputId": "7acd6eb7-12d3-45fd-94b1-30fc0fd1b25f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 20,  5, ..., 26, 14, 38], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_predict = model.predict(samples, lengths)\n",
    "pos_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CZUVE2n1BVI"
   },
   "source": [
    "The hmmlearn predict function will give the best probable path for the given sentence using the Viterbi algorithm.\n",
    "\n",
    "## Task 2: Using the model parameters (startprob_, transmat_, emissionprob_) write the viterbi algorithm from scratch to calculate the best probable path and compare it with the hmmlearn implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G279w_RtjJ-"
   },
   "source": [
    "Now before using these matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Viterbi(pi: np.array, a: np.array, b: np.array, obs: List) -> np.array():\n",
    "def Viterbi(pi, a, b, obs):\n",
    "    \"\"\"\n",
    "    Write the viterbi algorithm from scratch to find the best probable path\n",
    "    attr:\n",
    "        pi: initial probabilities\n",
    "        a: transition probabilities\n",
    "        b: emission probabilities\n",
    "        obs: list of observations\n",
    "    return:\n",
    "        array of the indices of the best hidden states\n",
    "    \"\"\"\n",
    "    n_state = len(pi)\n",
    "    n_transit = len(obs)\n",
    "    delta = np.zeros((n_transit, n_state))\n",
    "    phi = np.zeros((n_transit, n_state), dtype=int)\n",
    "\n",
    "    # Init\n",
    "    for i in range(n_state):\n",
    "        delta[0][i] = pi[i] * b[i][obs[0]]\n",
    "        phi[0][i] = 0\n",
    "\n",
    "    # Induction\n",
    "    for t in range(n_transit-1):\n",
    "        for j in range(n_state):\n",
    "            delta[t+1][j] = np.max(np.array([delta[t][i] * a[i][j] for i in range(n_state)])) * b[j][obs[t+1]]\n",
    "            phi[t+1][j] = np.argmax(np.array([delta[t][i] * a[i][j] for i in range(n_state)]))\n",
    "    \n",
    "    # Final\n",
    "    p_ml = np.max(delta[n_transit-1])\n",
    "    q_ml = np.zeros(n_transit, dtype=int)\n",
    "    q_ml[n_transit-1] = np.argmax(delta[n_transit-1])\n",
    "    for t in np.arange(n_transit-2, -1, -1, dtype=int):\n",
    "        q_ml[t] = phi[t+1][q_ml[t+1]]\n",
    "    \n",
    "    return q_ml, p_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']\n",
      "[ 5 20  5  2  1 20 26 18 36 39 14 20 26  0 36 39 14 20 28  5 20 39 14 38]\n",
      "[ 5 20  5  2  1 20 26 18 36 39 14 20 26  0 36 39 14 20 28  5 20 39 14 38]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\"\n",
    "q_ml, p_ml = Viterbi(pi=startprob, a=transmat, b=emissionprob, obs=sentence2idlist(test_sentence))\n",
    "print([id2tag[q] for q in q_ml])\n",
    "print(q_ml)\n",
    "print(pos_predict[0:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnHjHcLJ4Kz1"
   },
   "source": [
    "### Task 3: Let's try to form our own HMM\n",
    "In this task you will try to formulate your own HMM. Image a toy example that you think that closely relates to a Hidden Markov Model.\n",
    "\n",
    "Steps:\n",
    " 1. Define your hidden states\n",
    " 2. Define your observable states\n",
    " 3. Randomly generate your observations\n",
    "\n",
    "Below is an example to demonstrate:\n",
    "\n",
    "-In this toy HMM example, we have two hidden states 'healthy' and 'sick' these states relate to the state of a pet. In this example we cannot exactly know the situation of the pet if it is 'healthy' or 'sick'\n",
    "\n",
    "-The observable states in this formulation is the what our pet is doing, whether it is sleeping, eating or pooping. We ideally want to determine if the pet is sick or not using these observable states\n",
    "\n",
    "\n",
    "```python\n",
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observations = []\n",
    "for i in range(100):\n",
    "  observations.append(random.choice(observable_states))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2WAl5Pw7Oud"
   },
   "source": [
    "TASK 3: Now try to formulate your HMM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "hidden_states = ['hot', 'mild', 'cold']\n",
    "observable_states = ['dress', 't-shirt', 'coat']\n",
    "\n",
    "# make dataset\n",
    "N = 100\n",
    "states = []\n",
    "observations = []\n",
    "for i in range(N):\n",
    "    states.append(random.choice(hidden_states))\n",
    "\n",
    "for s in states:\n",
    "    rnd = random.random()\n",
    "    if s == 'hot':\n",
    "        if rnd < 0.60:\n",
    "            observations.append(random.choice(['t-shirt']))\n",
    "        else:\n",
    "            observations.append(random.choice(['dress']))\n",
    "    elif s == 'mild':\n",
    "        if rnd < 0.60:\n",
    "            observations.append(random.choice(['dress', 't-shirt']))\n",
    "        else:\n",
    "            observations.append(random.choice(['coat']))\n",
    "    else:\n",
    "        if rnd < 0.10:\n",
    "            observations.append(random.choice(['dress', 't-shirt']))\n",
    "        else:\n",
    "            observations.append(random.choice(['coat']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs2id = {o: i for i,o in enumerate(observable_states)}\n",
    "sta2id = {s: i for i,s in enumerate(hidden_states)}\n",
    "id2sta = {i: s for i,s in enumerate(hidden_states)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n",
      "[[0.36842105 0.23684211 0.39473684]\n",
      " [0.44444444 0.2962963  0.25925926]\n",
      " [0.35294118 0.32352941 0.32352941]]\n",
      "[[0.34210526 0.65789474 0.        ]\n",
      " [0.42857143 0.25       0.32142857]\n",
      " [0.05882353 0.08823529 0.85294118]]\n"
     ]
    }
   ],
   "source": [
    "startprobs = np.zeros(len(hidden_states))\n",
    "startprobs[sta2id[states[0]]] = 1\n",
    "\n",
    "transitprobmat = np.zeros((len(hidden_states),len(hidden_states)))\n",
    "for t in range(N):\n",
    "    if t == 0:\n",
    "        prevstate = sta2id[states[t]]\n",
    "        continue\n",
    "    transitprobmat[prevstate][sta2id[states[t]]] += 1\n",
    "    prevstate = sta2id[states[t]]\n",
    "for i in range(len(hidden_states)):\n",
    "    if np.sum(transitprobmat[i]) != 0:\n",
    "        transitprobmat[i] /= np.sum(transitprobmat[i])\n",
    "\n",
    "emissionprobmat = np.zeros((len(hidden_states),len(observable_states)))\n",
    "for t in range(N):\n",
    "    emissionprobmat[sta2id[states[t]]][obs2id[observations[t]]] += 1\n",
    "for i in range(len(observable_states)):\n",
    "    if np.sum(emissionprobmat[i]) != 0:\n",
    "        emissionprobmat[i] /= np.sum(emissionprobmat[i])\n",
    "\n",
    "print(startprobs)\n",
    "print(transitprobmat)\n",
    "print(emissionprobmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ourmodel = hmm.MultinomialHMM(n_components=len(hidden_states), algorithm='viterbi', random_state=42)\n",
    "ourmodel.startprob_ = startprobs # pi\n",
    "ourmodel.transmat_ = transitprobmat # A\n",
    "ourmodel.emissionprob_ = emissionprobmat # B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t-shirt', 'dress', 't-shirt', 'coat', 'coat', 't-shirt', 'coat', 'coat', 'coat', 'dress']\n",
      "['cold', 'mild', 'hot', 'cold', 'cold', 'hot', 'cold', 'cold', 'cold', 'mild']\n"
     ]
    }
   ],
   "source": [
    "test_obs = [random.choice(observable_states) for i in range(10)]\n",
    "print(test_obs)\n",
    "test_obs_id = [obs2id[obs] for obs in test_obs]\n",
    "pos_predict = ourmodel.predict([test_obs_id], 1)\n",
    "print([id2sta[p] for p in pos_predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWCJyGjS7hKp"
   },
   "source": [
    "Even tough we have generated the data randomly, for the learning purposes, let's try to learn an HMM from this data. For this we have to construct the Baum-Welch algorithm from scratch. Below is the skeleton of the Baum-Welch learning algorithm.\n",
    "\n",
    "## TASK 4: Complete the forward and backward probs functions in the Baum-Welch algorithm and try it with your formulated HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8571,
     "status": "ok",
     "timestamp": 1677623130291,
     "user": {
      "displayName": "Sri Kalidindi",
      "userId": "03508607547479251006"
     },
     "user_tz": -60
    },
    "id": "v6mJpFUg7K2V",
    "outputId": "0929edcc-5965-41e3-8b64-971085940f91",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:12<00:00, 160.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def baum_welch(observations, observations_vocab, n_hidden_states):\n",
    "    \"\"\"\n",
    "    Baum-Welch algorithm for estimating the HMM parameters\n",
    "    :param observations: observations\n",
    "    :param observations_vocab: observations vocabulary\n",
    "    :param n_hidden_states: number of hidden states to estimate\n",
    "    :return: a, b (transition matrix and emission matrix)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        forward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined alpha_\n",
    "        \"\"\"\n",
    "        a_start = 1 / n_hidden_states\n",
    "        alpha_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "        #TODO complete the forward function to calculate alpha\n",
    "        n_transit = len(observations)\n",
    "        \n",
    "        # Init\n",
    "        for i in range(n_hidden_states):\n",
    "            alpha_[i][0] = a_start * b_[i][observations[0]]\n",
    "\n",
    "        # Induction\n",
    "        for t in range(n_transit-1):\n",
    "            for j in range(n_hidden_states):\n",
    "                alpha_[j][t+1] = np.sum(np.array([alpha_[i][t] * a_[i][j] for i in range(n_hidden_states)])) * b_[j][observations[t+1]]\n",
    "\n",
    "        return alpha_\n",
    "\n",
    "    def backward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "        backward pass to calculate alpha\n",
    "        :param observations: observations\n",
    "        :param observations_vocab: observation vocabulary\n",
    "        :param n_hidden_states: number of hidden states\n",
    "        :param a_: estimated alpha\n",
    "        :param b_: estimated beta\n",
    "        :return: refined beta_\n",
    "        \"\"\"\n",
    "        beta_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
    "        beta_[:, -1:] = 1\n",
    "        # TODO finish the function to calculate backward pass and calculate beta\n",
    "        n_transit = len(observations)\n",
    "\n",
    "        # Induction\n",
    "        for t in np.arange(n_transit-2, -1, -1):\n",
    "            for j in range(n_hidden_states):\n",
    "                beta_[j][t] = np.sum([a_[j][i] * b[i][observations[t+1]] * beta_[i][t+1] for i in range(n_hidden_states)])\n",
    "\n",
    "        return beta_\n",
    "\n",
    "    def compute_gamma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # gamma_prob = np.zeros(n_samples, len(observations))\n",
    "        gamma_prob = np.multiply(alfa, beta) / sum(np.multiply(alfa, beta))\n",
    "        return gamma_prob\n",
    "\n",
    "    def compute_sigma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param alfa:\n",
    "        :param beta:\n",
    "        :param observations:\n",
    "        :param vocab:\n",
    "        :param n_samples:\n",
    "        :param a_:\n",
    "        :param b_:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sigma_prob = np.zeros((n_samples, len(observations) - 1, n_samples), dtype=float)\n",
    "        denomenator = np.multiply(alfa, beta)\n",
    "        for i in range(len(observations) - 1):\n",
    "            for j in range(n_samples):\n",
    "                for k in range(n_samples):\n",
    "                    index_in_vocab = np.where(vocab == observations[i + 1])[0][0]\n",
    "                    sigma_prob[j, i, k] = (alfa[j, i] * beta[k, i + 1] * a_[j, k] * b_[k, index_in_vocab]) / sum(\n",
    "                        denomenator[:, j])\n",
    "        return sigma_prob\n",
    "\n",
    "    # initialize A ,B\n",
    "    a = np.ones((n_hidden_states, n_hidden_states)) / n_hidden_states\n",
    "    b = np.ones((n_hidden_states, len(observations_vocab))) / len(observations_vocab)\n",
    "    for iter in tqdm(range(2000), position=0, leave=True):\n",
    "\n",
    "        # E-step caclculating sigma and gamma\n",
    "        alfa_prob = forward_probs(observations, observations_vocab, n_hidden_states, a, b)  #\n",
    "        beta_prob = backward_probs(observations, observations_vocab, n_hidden_states, a, b)  # , beta_val\n",
    "        gamma_prob = compute_gamma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "        sigma_prob = compute_sigma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
    "\n",
    "        # M-step caclculating A, B matrices\n",
    "        a_model = np.zeros((n_hidden_states, n_hidden_states))\n",
    "        for j in range(n_hidden_states):  # calculate A-model\n",
    "            for i in range(n_hidden_states):\n",
    "                for t in range(len(observations) - 1):\n",
    "                    a_model[j, i] = a_model[j, i] + sigma_prob[j, t, i]\n",
    "                normalize_a = [sigma_prob[j, t_current, i_current] for t_current in range(len(observations) - 1) for\n",
    "                               i_current in range(n_hidden_states)]\n",
    "                normalize_a = sum(normalize_a)\n",
    "                if normalize_a == 0:\n",
    "                    a_model[j, i] = 0\n",
    "                else:\n",
    "                    a_model[j, i] = a_model[j, i] / normalize_a\n",
    "\n",
    "        b_model = np.zeros((n_hidden_states, len(observations_vocab)))\n",
    "\n",
    "        for j in range(n_hidden_states):\n",
    "            for i in range(len(observations_vocab)):\n",
    "                indices = [idx for idx, val in enumerate(observations) if val == observations_vocab[i]]\n",
    "                numerator_b = sum(gamma_prob[j, indices])\n",
    "                denominator_b = sum(gamma_prob[j, :])\n",
    "                if denominator_b == 0:\n",
    "                    b_model[j, i] = 0\n",
    "                else:\n",
    "                    b_model[j, i] = numerator_b / denominator_b\n",
    "\n",
    "        a = a_model\n",
    "        b = b_model\n",
    "    return a, b\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(40)\n",
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observable_map = {'sleeping': 0, 'eating': 1, 'pooping': 2}\n",
    "observations = []\n",
    "for i in range(100):\n",
    "    observations.append(observable_map[random.choice(observable_states)])\n",
    "# print(observations)\n",
    "# print(np.array(list(observable_map.values())))\n",
    "\n",
    "A, B = baum_welch(observations=observations, observations_vocab=np.array(list(observable_map.values())),\n",
    "                  n_hidden_states=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.36 0.31 0.33]\n",
      " [0.36 0.31 0.33]]\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "hidden_states = ['hot', 'mild', 'cold']\n",
    "observable_states = ['dress', 't-shirt', 'coat']\n",
    "\n",
    "# make dataset\n",
    "N = 100\n",
    "states = []\n",
    "observations = []\n",
    "for i in range(N):\n",
    "    states.append(random.choice(hidden_states))\n",
    "\n",
    "for s in states:\n",
    "    rnd = random.random()\n",
    "    if s == 'hot':\n",
    "        if rnd < 0.60:\n",
    "            observations.append(random.choice(['t-shirt']))\n",
    "        else:\n",
    "            observations.append(random.choice(['dress']))\n",
    "    elif s == 'mild':\n",
    "        if rnd < 0.60:\n",
    "            observations.append(random.choice(['dress', 't-shirt']))\n",
    "        else:\n",
    "            observations.append(random.choice(['coat']))\n",
    "    else:\n",
    "        if rnd < 0.10:\n",
    "            observations.append(random.choice(['dress', 't-shirt']))\n",
    "        else:\n",
    "            observations.append(random.choice(['coat']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "SJzrpdwJEf0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:25<00:00, 78.24it/s]\n"
     ]
    }
   ],
   "source": [
    "#TASK 4: Now try it with your HMM\n",
    "observations = [obs2id[obs] for obs in observations]\n",
    "A, B = baum_welch(observations=observations, observations_vocab=np.arange(0, len(observable_states)), n_hidden_states=len(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "[[0.27 0.35 0.38]\n",
      " [0.27 0.35 0.38]\n",
      " [0.27 0.35 0.38]]\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print(B)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1zewbaLQpah4tXjgkum-DIYJX04_gCbEc",
     "timestamp": 1677666490463
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}